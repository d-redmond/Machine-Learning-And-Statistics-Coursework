{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84623d7e",
   "metadata": {},
   "source": [
    "# Timeseries Anomaly Detection\n",
    "\n",
    "keras documentation: https://keras.io/examples/timeseries/timeseries_anomaly_detection/\n",
    "\n",
    "github source code: https://github.com/keras-team/keras-io/blob/master/examples/timeseries/timeseries_anomaly_detection.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15444a76",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e870ac2",
   "metadata": {},
   "source": [
    "This notebook focuses on Keras documentation code (linked above) on timeseries anomaly detection using an Autoencoder. The purpose of this script is to demonstrate how to use a reconstruction convultional autoencoder model to detect anomalies in timeseries data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb74c3a",
   "metadata": {},
   "source": [
    "Keras is a deep learning API developed by Google for implementing neural networks. \n",
    "\n",
    "A timeseries is a collection of data points gathered over time intervals that allow tracking changes over time. It is particulary useful for identifying anomalies as the rate is generally less than 1%.\n",
    "\n",
    "Anomaly detection is concerned with outlier detection. IT identifies the differences, deviations, and exceptions from the norm in a timeseries.\n",
    "\n",
    "An autoencoder is a type of artificial nerual network where the input is the same as the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd623e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries: numerical, dataframes, neural networks, plotting, visualization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de92394",
   "metadata": {},
   "source": [
    "Keras documentation provides two Numenta Anomaly Benchmark datasets. It provides artificial timeseries data containing labeled anomalous periods of behaviour. Data is ordered, timestamped, and of single-valued metric. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bd9856",
   "metadata": {},
   "source": [
    "Keras documentation suggests use of the art_daily_small_noise.csv file for training and the art_daily_jumpsup.csv file for testing. The simplicity of this dataset allows us to demonstrate anomaly detection effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6925fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training files: master root url\n",
    "master_url_root = \"https://raw.githubusercontent.com/numenta/NAB/master/data/\"\n",
    "\n",
    "# Data files without anomalies\n",
    "df_small_noise_url_suffix = \"artificialNoAnomaly/art_daily_small_noise.csv\"\n",
    "df_small_noise_url = master_url_root + df_small_noise_url_suffix\n",
    "df_small_noise = pd.read_csv(\n",
    "    df_small_noise_url, parse_dates=True, index_col=\"timestamp\"\n",
    ")\n",
    "\n",
    "# Data files with anomalies\n",
    "df_daily_jumpsup_url_suffix = \"artificialWithAnomaly/art_daily_jumpsup.csv\"\n",
    "df_daily_jumpsup_url = master_url_root + df_daily_jumpsup_url_suffix\n",
    "df_daily_jumpsup = pd.read_csv(\n",
    "    df_daily_jumpsup_url, parse_dates=True, index_col=\"timestamp\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08132d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brief look at the data\n",
    "print(df_small_noise.head())\n",
    "print(df_daily_jumpsup.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28992120",
   "metadata": {},
   "source": [
    "### Visualizing the Data: Timeseries data without anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d6a39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "fig, ax = plt.subplots()\n",
    "df_small_noise.plot(legend=False, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c130a973",
   "metadata": {},
   "source": [
    "### Visualizing the Data: Timeseries data with anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f8a439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing data\n",
    "fig, ax = plt.subplots()\n",
    "df_daily_jumpsup.plot(legend=False, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86efb195",
   "metadata": {},
   "source": [
    "### Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c0d80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize and save the mean and std we get,\n",
    "# for normalizing test data.\n",
    "training_mean = df_small_noise.mean()\n",
    "training_std = df_small_noise.std()\n",
    "df_training_value = (df_small_noise - training_mean) / training_std\n",
    "print(\"Number of training samples:\", len(df_training_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e128461f",
   "metadata": {},
   "source": [
    "### Create Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa47747",
   "metadata": {},
   "source": [
    "Create sequences combining `TIME_STEPS` contiguous data values from the\n",
    "training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ea0028",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_STEPS = 288\n",
    "\n",
    "# Generated training sequences for use in the model.\n",
    "def create_sequences(values, time_steps=TIME_STEPS):\n",
    "    output = []\n",
    "    for i in range(len(values) - time_steps + 1):\n",
    "        output.append(values[i : (i + time_steps)])\n",
    "    return np.stack(output)\n",
    "\n",
    "\n",
    "x_train = create_sequences(df_training_value.values)\n",
    "print(\"Training input shape: \", x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a2e83f",
   "metadata": {},
   "source": [
    "### Build a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1755290a",
   "metadata": {},
   "source": [
    "We will build a convolutional reconstruction autoencoder model. The model will\n",
    "take input of shape `(batch_size, sequence_length, num_features)` and return\n",
    "output of the same shape. In this case, `sequence_length` is 288 and\n",
    "`num_features` is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090cdbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Input(shape=(x_train.shape[1], x_train.shape[2])),\n",
    "        layers.Conv1D(\n",
    "            filters=32, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"\n",
    "        ),\n",
    "        layers.Dropout(rate=0.2),\n",
    "        layers.Conv1D(\n",
    "            filters=16, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"\n",
    "        ),\n",
    "        layers.Conv1DTranspose(\n",
    "            filters=16, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"\n",
    "        ),\n",
    "        layers.Dropout(rate=0.2),\n",
    "        layers.Conv1DTranspose(\n",
    "            filters=32, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"\n",
    "        ),\n",
    "        layers.Conv1DTranspose(filters=1, kernel_size=7, padding=\"same\"),\n",
    "    ]\n",
    ")\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss=\"mse\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baefef6a",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6f7ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x_train,\n",
    "    x_train,\n",
    "    epochs=50,\n",
    "    batch_size=128,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, mode=\"min\")\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220b4983",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's plot training and validation loss to see how the training went.\n",
    "\"\"\"\n",
    "\n",
    "plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b82d55",
   "metadata": {},
   "source": [
    "### Detecting Anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8295da",
   "metadata": {},
   "source": [
    "We will detect anomalies by determining how well our model can reconstruct\n",
    "the input data.\n",
    "1.   Find MAE loss on training samples.\n",
    "2.   Find max MAE loss value. This is the worst our model has performed trying\n",
    "to reconstruct a sample. We will make this the `threshold` for anomaly\n",
    "detection.\n",
    "3.   If the reconstruction loss for a sample is greater than this `threshold`\n",
    "value then we can infer that the model is seeing a pattern that it isn't\n",
    "familiar with. We will label this sample as an `anomaly`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa945e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train MAE loss.\n",
    "x_train_pred = model.predict(x_train)\n",
    "train_mae_loss = np.mean(np.abs(x_train_pred - x_train), axis=1)\n",
    "\n",
    "plt.hist(train_mae_loss, bins=50)\n",
    "plt.xlabel(\"Train MAE loss\")\n",
    "plt.ylabel(\"No of samples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24771210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get reconstruction loss threshold.\n",
    "threshold = np.max(train_mae_loss)\n",
    "print(\"Reconstruction error threshold: \", threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9609922",
   "metadata": {},
   "source": [
    "### Compare Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9400727",
   "metadata": {},
   "outputs": [],
   "source": [
    "Just for fun, let's see how our model has recontructed the first sample.\n",
    "This is the 288 timesteps from day 1 of our training dataset.\n",
    "\"\"\"\n",
    "\n",
    "# Checking how the first sequence is learnt\n",
    "plt.plot(x_train[0])\n",
    "plt.plot(x_train_pred[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7fe540",
   "metadata": {},
   "source": [
    "### Prepare the Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce1dd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_value = (df_daily_jumpsup - training_mean) / training_std\n",
    "fig, ax = plt.subplots()\n",
    "df_test_value.plot(legend=False, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bf341c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences from test values.\n",
    "x_test = create_sequences(df_test_value.values)\n",
    "print(\"Test input shape: \", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41cd1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test MAE loss.\n",
    "x_test_pred = model.predict(x_test)\n",
    "test_mae_loss = np.mean(np.abs(x_test_pred - x_test), axis=1)\n",
    "test_mae_loss = test_mae_loss.reshape((-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e788c80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(test_mae_loss, bins=50)\n",
    "plt.xlabel(\"test MAE loss\")\n",
    "plt.ylabel(\"No of samples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44e06ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect all the samples which are anomalies.\n",
    "anomalies = test_mae_loss > threshold\n",
    "print(\"Number of anomaly samples: \", np.sum(anomalies))\n",
    "print(\"Indices of anomaly samples: \", np.where(anomalies))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80529ef",
   "metadata": {},
   "source": [
    "### Plot Anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1af7b42",
   "metadata": {},
   "source": [
    "We now know the samples of the data which are anomalies. With this, we will\n",
    "find the corresponding `timestamps` from the original test data. We will be\n",
    "using the following method to do that:\n",
    "Let's say time_steps = 3 and we have 10 training values. Our `x_train` will\n",
    "look like this:\n",
    "- 0, 1, 2\n",
    "- 1, 2, 3\n",
    "- 2, 3, 4\n",
    "- 3, 4, 5\n",
    "- 4, 5, 6\n",
    "- 5, 6, 7\n",
    "- 6, 7, 8\n",
    "- 7, 8, 9\n",
    "All except the initial and the final time_steps-1 data values, will appear in\n",
    "`time_steps` number of samples. So, if we know that the samples\n",
    "[(3, 4, 5), (4, 5, 6), (5, 6, 7)] are anomalies, we can say that the data point\n",
    "5 is an anomaly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0b2edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data i is an anomaly if samples [(i - timesteps + 1) to (i)] are anomalies\n",
    "anomalous_data_indices = []\n",
    "for data_idx in range(TIME_STEPS - 1, len(df_test_value) - TIME_STEPS + 1):\n",
    "    if np.all(anomalies[data_idx - TIME_STEPS + 1 : data_idx]):\n",
    "        anomalous_data_indices.append(data_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e00ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overlay the anomalies on the original test data plot\n",
    "\n",
    "df_subset = df_daily_jumpsup.iloc[anomalous_data_indices]\n",
    "fig, ax = plt.subplots()\n",
    "df_daily_jumpsup.plot(legend=False, ax=ax)\n",
    "df_subset.plot(legend=False, ax=ax, color=\"r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bd28e3",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678b0c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://keras.io/examples/timeseries/timeseries_anomaly_detection/\n",
    "    \n",
    "https://github.com/Charlie5DH/Anomaly-Detection-in-time-series\n",
    "    \n",
    "https://pub.towardsai.net/autoencoder-for-anomaly-detection-using-tensorflow-keras-7fdfa9f3ad99"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
